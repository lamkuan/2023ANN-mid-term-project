{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9efb2547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "是否使用GPU训练：True\n",
      "{'n000013': 0, 'n000025': 1, 'n000136': 2, 'n000146': 3, 'n000167': 4, 'n000172': 5, 'n000267': 6, 'n000268': 7, 'n000270': 8, 'n000272': 9, 'n000281': 10, 'n000302': 11, 'n000310': 12, 'n000320': 13, 'n000325': 14, 'n000336': 15, 'n000360': 16, 'n000390': 17, 'n000399': 18, 'n000411': 19, 'n000418': 20, 'n000426': 21, 'n000454': 22, 'n000462': 23, 'n000463': 24, 'n000476': 25, 'n000515': 26, 'n000526': 27, 'n000549': 28, 'n000570': 29, 'n000575': 30, 'n000586': 31, 'n000613': 32, 'n000641': 33, 'n000642': 34, 'n000655': 35, 'n000656': 36, 'n000674': 37, 'n000682': 38, 'n000696': 39, 'n000700': 40, 'n000716': 41, 'n000731': 42, 'n000737': 43, 'n000788': 44, 'n000805': 45, 'n000855': 46, 'n000870': 47, 'n000897': 48, 'n000908': 49, 'n000924': 50, 'n000941': 51, 'n000966': 52, 'n000994': 53, 'n001012': 54, 'n001043': 55, 'n001082': 56, 'n001123': 57, 'n001139': 58, 'n001148': 59, 'n001169': 60, 'n001179': 61, 'n001188': 62, 'n001204': 63, 'n001231': 64, 'n001261': 65, 'n001298': 66, 'n001306': 67, 'n001310': 68, 'n001340': 69, 'n001356': 70, 'n001359': 71, 'n001385': 72, 'n001398': 73, 'n001438': 74, 'n001455': 75, 'n001459': 76, 'n001460': 77, 'n001462': 78, 'n001478': 79, 'n001491': 80, 'n001498': 81, 'n001514': 82, 'n001532': 83, 'n001547': 84, 'n001552': 85, 'n001557': 86, 'n001560': 87, 'n001567': 88, 'n001570': 89, 'n001592': 90, 'n001615': 91, 'n001645': 92, 'n001666': 93, 'n001754': 94, 'n001757': 95, 'n001766': 96, 'n001780': 97, 'n001789': 98, 'n001808': 99, 'n001809': 100, 'n001865': 101, 'n001879': 102, 'n001899': 103, 'n001903': 104, 'n001974': 105, 'n001989': 106, 'n002016': 107, 'n002037': 108, 'n002038': 109, 'n002043': 110, 'n002057': 111, 'n002065': 112, 'n002073': 113, 'n002094': 114, 'n002113': 115, 'n002117': 116, 'n002208': 117, 'n002220': 118, 'n002224': 119, 'n002226': 120, 'n002259': 121, 'n002276': 122, 'n002277': 123, 'n002305': 124, 'n002307': 125, 'n002314': 126, 'n002339': 127, 'n002368': 128, 'n002391': 129, 'n002416': 130, 'n002427': 131, 'n002457': 132, 'n002472': 133, 'n002500': 134, 'n002525': 135, 'n002527': 136, 'n002538': 137, 'n002543': 138, 'n002551': 139, 'n002578': 140, 'n002632': 141, 'n002634': 142, 'n002650': 143, 'n002711': 144, 'n002713': 145, 'n002794': 146, 'n002802': 147, 'n002804': 148, 'n002807': 149, 'n002809': 150, 'n002817': 151, 'n002818': 152, 'n002821': 153, 'n002830': 154, 'n002849': 155, 'n002867': 156, 'n002924': 157, 'n002928': 158, 'n002957': 159, 'n002962': 160, 'n002974': 161, 'n002984': 162, 'n003016': 163, 'n003077': 164, 'n003107': 165, 'n003132': 166, 'n003135': 167, 'n003147': 168, 'n003152': 169, 'n003171': 170, 'n003185': 171, 'n003190': 172, 'n003204': 173, 'n003206': 174, 'n003208': 175, 'n003212': 176, 'n003245': 177, 'n003253': 178, 'n003267': 179, 'n003284': 180, 'n003291': 181, 'n003299': 182, 'n003308': 183, 'n003348': 184, 'n003350': 185, 'n003353': 186, 'n003362': 187, 'n003367': 188, 'n003370': 189, 'n003385': 190, 'n003392': 191, 'n003401': 192, 'n003409': 193, 'n003413': 194, 'n003417': 195, 'n003453': 196, 'n003454': 197, 'n003470': 198, 'n003474': 199, 'n003494': 200, 'n003511': 201, 'n003549': 202, 'n003555': 203, 'n003581': 204, 'n003595': 205, 'n003605': 206, 'n003620': 207, 'n003627': 208, 'n003678': 209, 'n003695': 210, 'n003724': 211, 'n003737': 212, 'n003740': 213, 'n003752': 214, 'n003770': 215, 'n003865': 216, 'n003883': 217, 'n003884': 218, 'n003906': 219, 'n003963': 220, 'n003974': 221, 'n003975': 222, 'n003997': 223, 'n004018': 224, 'n004032': 225, 'n004071': 226, 'n004088': 227, 'n004107': 228, 'n004113': 229, 'n004116': 230, 'n004132': 231, 'n004171': 232, 'n004221': 233, 'n004232': 234, 'n004302': 235, 'n004325': 236, 'n004326': 237, 'n004337': 238, 'n004381': 239, 'n004410': 240, 'n004421': 241, 'n004438': 242, 'n004446': 243, 'n004463': 244, 'n004493': 245, 'n004544': 246, 'n004567': 247, 'n004575': 248, 'n004590': 249, 'n004601': 250, 'n004609': 251, 'n004611': 252, 'n004612': 253, 'n004644': 254, 'n004770': 255, 'n004782': 256, 'n004802': 257, 'n004820': 258, 'n004822': 259, 'n004848': 260, 'n004872': 261, 'n004893': 262, 'n004901': 263, 'n004988': 264, 'n004994': 265, 'n005002': 266, 'n005045': 267, 'n005065': 268, 'n005067': 269, 'n005081': 270, 'n005097': 271, 'n005121': 272, 'n005127': 273, 'n005144': 274, 'n005197': 275, 'n005219': 276, 'n005265': 277, 'n005318': 278, 'n005320': 279, 'n005325': 280, 'n005357': 281, 'n005363': 282, 'n005402': 283, 'n005443': 284, 'n005469': 285, 'n005506': 286, 'n005514': 287, 'n005547': 288, 'n005550': 289, 'n005554': 290, 'n005566': 291, 'n005580': 292, 'n005583': 293, 'n005584': 294, 'n005588': 295, 'n005596': 296, 'n005655': 297, 'n005657': 298, 'n005674': 299, 'n005684': 300, 'n005708': 301, 'n005718': 302, 'n005734': 303, 'n005741': 304, 'n005767': 305, 'n005805': 306, 'n005813': 307, 'n005823': 308, 'n005850': 309, 'n005858': 310, 'n005869': 311, 'n005890': 312, 'n005898': 313, 'n005954': 314, 'n005972': 315, 'n006015': 316, 'n006056': 317, 'n006072': 318, 'n006081': 319, 'n006091': 320, 'n006095': 321, 'n006108': 322, 'n006117': 323, 'n006126': 324, 'n006167': 325, 'n006180': 326, 'n006181': 327, 'n006187': 328, 'n006197': 329, 'n006204': 330, 'n006205': 331, 'n006235': 332, 'n006253': 333, 'n006281': 334, 'n006298': 335, 'n006303': 336, 'n006329': 337, 'n006345': 338, 'n006354': 339, 'n006357': 340, 'n006364': 341, 'n006386': 342, 'n006437': 343, 'n006445': 344, 'n006455': 345, 'n006456': 346, 'n006457': 347, 'n006498': 348, 'n006499': 349, 'n006500': 350, 'n006509': 351, 'n006528': 352, 'n006537': 353, 'n006547': 354, 'n006557': 355, 'n006609': 356, 'n006614': 357, 'n006623': 358, 'n006652': 359, 'n006697': 360, 'n006715': 361, 'n006726': 362, 'n006733': 363, 'n006771': 364, 'n006798': 365, 'n006806': 366, 'n006809': 367, 'n006822': 368, 'n006836': 369, 'n006837': 370, 'n006875': 371, 'n006894': 372, 'n006901': 373, 'n006926': 374, 'n006932': 375, 'n006961': 376, 'n006972': 377, 'n006975': 378, 'n007009': 379, 'n007010': 380, 'n007046': 381, 'n007085': 382, 'n007100': 383, 'n007108': 384, 'n007114': 385, 'n007155': 386, 'n007178': 387, 'n007197': 388, 'n007255': 389, 'n007259': 390, 'n007273': 391, 'n007279': 392, 'n007287': 393, 'n007304': 394, 'n007325': 395, 'n007327': 396, 'n007333': 397, 'n007340': 398, 'n007398': 399, 'n007464': 400, 'n007473': 401, 'n007529': 402, 'n007535': 403, 'n007559': 404, 'n007568': 405, 'n007572': 406, 'n007576': 407, 'n007629': 408, 'n007635': 409, 'n007652': 410, 'n007658': 411, 'n007684': 412, 'n007708': 413, 'n007732': 414, 'n007744': 415, 'n007755': 416, 'n007771': 417, 'n007789': 418, 'n007801': 419, 'n007813': 420, 'n007821': 421, 'n007831': 422, 'n007853': 423, 'n007867': 424, 'n007925': 425, 'n007932': 426, 'n007933': 427, 'n007935': 428, 'n007958': 429, 'n007992': 430, 'n008018': 431, 'n008019': 432, 'n008038': 433, 'n008099': 434, 'n008116': 435, 'n008141': 436, 'n008156': 437, 'n008172': 438, 'n008204': 439, 'n008210': 440, 'n008219': 441, 'n008221': 442, 'n008230': 443, 'n008242': 444, 'n008244': 445, 'n008250': 446, 'n008257': 447, 'n008267': 448, 'n008280': 449, 'n008319': 450, 'n008330': 451, 'n008353': 452, 'n008406': 453, 'n008418': 454, 'n008428': 455, 'n008460': 456, 'n008467': 457, 'n008490': 458, 'n008522': 459, 'n008531': 460, 'n008534': 461, 'n008623': 462, 'n008649': 463, 'n008667': 464, 'n008675': 465, 'n008693': 466, 'n008719': 467, 'n008750': 468, 'n008751': 469, 'n008786': 470, 'n008788': 471, 'n008795': 472, 'n008797': 473, 'n008842': 474, 'n008860': 475, 'n008881': 476, 'n008886': 477, 'n008906': 478, 'n008916': 479, 'n008950': 480, 'n008961': 481, 'n008975': 482, 'n008977': 483, 'n008982': 484, 'n008994': 485, 'n008996': 486, 'n009003': 487, 'n009049': 488, 'n009054': 489, 'n009061': 490, 'n009092': 491, 'n009095': 492, 'n009100': 493, 'n009142': 494, 'n009150': 495, 'n009179': 496, 'n009224': 497, 'n009230': 498, 'n009242': 499}\n",
      "{'n000013': 0, 'n000025': 1, 'n000136': 2, 'n000146': 3, 'n000167': 4, 'n000172': 5, 'n000267': 6, 'n000268': 7, 'n000270': 8, 'n000272': 9, 'n000281': 10, 'n000302': 11, 'n000310': 12, 'n000320': 13, 'n000325': 14, 'n000336': 15, 'n000360': 16, 'n000390': 17, 'n000399': 18, 'n000411': 19, 'n000418': 20, 'n000426': 21, 'n000454': 22, 'n000462': 23, 'n000463': 24, 'n000476': 25, 'n000515': 26, 'n000526': 27, 'n000549': 28, 'n000570': 29, 'n000575': 30, 'n000586': 31, 'n000613': 32, 'n000641': 33, 'n000642': 34, 'n000655': 35, 'n000656': 36, 'n000674': 37, 'n000682': 38, 'n000696': 39, 'n000700': 40, 'n000716': 41, 'n000731': 42, 'n000737': 43, 'n000788': 44, 'n000805': 45, 'n000855': 46, 'n000870': 47, 'n000897': 48, 'n000908': 49, 'n000924': 50, 'n000941': 51, 'n000966': 52, 'n000994': 53, 'n001012': 54, 'n001043': 55, 'n001082': 56, 'n001123': 57, 'n001139': 58, 'n001148': 59, 'n001169': 60, 'n001179': 61, 'n001188': 62, 'n001204': 63, 'n001231': 64, 'n001261': 65, 'n001298': 66, 'n001306': 67, 'n001310': 68, 'n001340': 69, 'n001356': 70, 'n001359': 71, 'n001385': 72, 'n001398': 73, 'n001438': 74, 'n001455': 75, 'n001459': 76, 'n001460': 77, 'n001462': 78, 'n001478': 79, 'n001491': 80, 'n001498': 81, 'n001514': 82, 'n001532': 83, 'n001547': 84, 'n001552': 85, 'n001557': 86, 'n001560': 87, 'n001567': 88, 'n001570': 89, 'n001592': 90, 'n001615': 91, 'n001645': 92, 'n001666': 93, 'n001754': 94, 'n001757': 95, 'n001766': 96, 'n001780': 97, 'n001789': 98, 'n001808': 99, 'n001809': 100, 'n001865': 101, 'n001879': 102, 'n001899': 103, 'n001903': 104, 'n001974': 105, 'n001989': 106, 'n002016': 107, 'n002037': 108, 'n002038': 109, 'n002043': 110, 'n002057': 111, 'n002065': 112, 'n002073': 113, 'n002094': 114, 'n002113': 115, 'n002117': 116, 'n002208': 117, 'n002220': 118, 'n002224': 119, 'n002226': 120, 'n002259': 121, 'n002276': 122, 'n002277': 123, 'n002305': 124, 'n002307': 125, 'n002314': 126, 'n002339': 127, 'n002368': 128, 'n002391': 129, 'n002416': 130, 'n002427': 131, 'n002457': 132, 'n002472': 133, 'n002500': 134, 'n002525': 135, 'n002527': 136, 'n002538': 137, 'n002543': 138, 'n002551': 139, 'n002578': 140, 'n002632': 141, 'n002634': 142, 'n002650': 143, 'n002711': 144, 'n002713': 145, 'n002794': 146, 'n002802': 147, 'n002804': 148, 'n002807': 149, 'n002809': 150, 'n002817': 151, 'n002818': 152, 'n002821': 153, 'n002830': 154, 'n002849': 155, 'n002867': 156, 'n002924': 157, 'n002928': 158, 'n002957': 159, 'n002962': 160, 'n002974': 161, 'n002984': 162, 'n003016': 163, 'n003077': 164, 'n003107': 165, 'n003132': 166, 'n003135': 167, 'n003147': 168, 'n003152': 169, 'n003171': 170, 'n003185': 171, 'n003190': 172, 'n003204': 173, 'n003206': 174, 'n003208': 175, 'n003212': 176, 'n003245': 177, 'n003253': 178, 'n003267': 179, 'n003284': 180, 'n003291': 181, 'n003299': 182, 'n003308': 183, 'n003348': 184, 'n003350': 185, 'n003353': 186, 'n003362': 187, 'n003367': 188, 'n003370': 189, 'n003385': 190, 'n003392': 191, 'n003401': 192, 'n003409': 193, 'n003413': 194, 'n003417': 195, 'n003453': 196, 'n003454': 197, 'n003470': 198, 'n003474': 199, 'n003494': 200, 'n003511': 201, 'n003549': 202, 'n003555': 203, 'n003581': 204, 'n003595': 205, 'n003605': 206, 'n003620': 207, 'n003627': 208, 'n003678': 209, 'n003695': 210, 'n003724': 211, 'n003737': 212, 'n003740': 213, 'n003752': 214, 'n003770': 215, 'n003865': 216, 'n003883': 217, 'n003884': 218, 'n003906': 219, 'n003963': 220, 'n003974': 221, 'n003975': 222, 'n003997': 223, 'n004018': 224, 'n004032': 225, 'n004071': 226, 'n004088': 227, 'n004107': 228, 'n004113': 229, 'n004116': 230, 'n004132': 231, 'n004171': 232, 'n004221': 233, 'n004232': 234, 'n004302': 235, 'n004325': 236, 'n004326': 237, 'n004337': 238, 'n004381': 239, 'n004410': 240, 'n004421': 241, 'n004438': 242, 'n004446': 243, 'n004463': 244, 'n004493': 245, 'n004544': 246, 'n004567': 247, 'n004575': 248, 'n004590': 249, 'n004601': 250, 'n004609': 251, 'n004611': 252, 'n004612': 253, 'n004644': 254, 'n004770': 255, 'n004782': 256, 'n004802': 257, 'n004820': 258, 'n004822': 259, 'n004848': 260, 'n004872': 261, 'n004893': 262, 'n004901': 263, 'n004988': 264, 'n004994': 265, 'n005002': 266, 'n005045': 267, 'n005065': 268, 'n005067': 269, 'n005081': 270, 'n005097': 271, 'n005121': 272, 'n005127': 273, 'n005144': 274, 'n005197': 275, 'n005219': 276, 'n005265': 277, 'n005318': 278, 'n005320': 279, 'n005325': 280, 'n005357': 281, 'n005363': 282, 'n005402': 283, 'n005443': 284, 'n005469': 285, 'n005506': 286, 'n005514': 287, 'n005547': 288, 'n005550': 289, 'n005554': 290, 'n005566': 291, 'n005580': 292, 'n005583': 293, 'n005584': 294, 'n005588': 295, 'n005596': 296, 'n005655': 297, 'n005657': 298, 'n005674': 299, 'n005684': 300, 'n005708': 301, 'n005718': 302, 'n005734': 303, 'n005741': 304, 'n005767': 305, 'n005805': 306, 'n005813': 307, 'n005823': 308, 'n005850': 309, 'n005858': 310, 'n005869': 311, 'n005890': 312, 'n005898': 313, 'n005954': 314, 'n005972': 315, 'n006015': 316, 'n006056': 317, 'n006072': 318, 'n006081': 319, 'n006091': 320, 'n006095': 321, 'n006108': 322, 'n006117': 323, 'n006126': 324, 'n006167': 325, 'n006180': 326, 'n006181': 327, 'n006187': 328, 'n006197': 329, 'n006204': 330, 'n006205': 331, 'n006235': 332, 'n006253': 333, 'n006281': 334, 'n006298': 335, 'n006303': 336, 'n006329': 337, 'n006345': 338, 'n006354': 339, 'n006357': 340, 'n006364': 341, 'n006386': 342, 'n006437': 343, 'n006445': 344, 'n006455': 345, 'n006456': 346, 'n006457': 347, 'n006498': 348, 'n006499': 349, 'n006500': 350, 'n006509': 351, 'n006528': 352, 'n006537': 353, 'n006547': 354, 'n006557': 355, 'n006609': 356, 'n006614': 357, 'n006623': 358, 'n006652': 359, 'n006697': 360, 'n006715': 361, 'n006726': 362, 'n006733': 363, 'n006771': 364, 'n006798': 365, 'n006806': 366, 'n006809': 367, 'n006822': 368, 'n006836': 369, 'n006837': 370, 'n006875': 371, 'n006894': 372, 'n006901': 373, 'n006926': 374, 'n006932': 375, 'n006961': 376, 'n006972': 377, 'n006975': 378, 'n007009': 379, 'n007010': 380, 'n007046': 381, 'n007085': 382, 'n007100': 383, 'n007108': 384, 'n007114': 385, 'n007155': 386, 'n007178': 387, 'n007197': 388, 'n007255': 389, 'n007259': 390, 'n007273': 391, 'n007279': 392, 'n007287': 393, 'n007304': 394, 'n007325': 395, 'n007327': 396, 'n007333': 397, 'n007340': 398, 'n007398': 399, 'n007464': 400, 'n007473': 401, 'n007529': 402, 'n007535': 403, 'n007559': 404, 'n007568': 405, 'n007572': 406, 'n007576': 407, 'n007629': 408, 'n007635': 409, 'n007652': 410, 'n007658': 411, 'n007684': 412, 'n007708': 413, 'n007732': 414, 'n007744': 415, 'n007755': 416, 'n007771': 417, 'n007789': 418, 'n007801': 419, 'n007813': 420, 'n007821': 421, 'n007831': 422, 'n007853': 423, 'n007867': 424, 'n007925': 425, 'n007932': 426, 'n007933': 427, 'n007935': 428, 'n007958': 429, 'n007992': 430, 'n008018': 431, 'n008019': 432, 'n008038': 433, 'n008099': 434, 'n008116': 435, 'n008141': 436, 'n008156': 437, 'n008172': 438, 'n008204': 439, 'n008210': 440, 'n008219': 441, 'n008221': 442, 'n008230': 443, 'n008242': 444, 'n008244': 445, 'n008250': 446, 'n008257': 447, 'n008267': 448, 'n008280': 449, 'n008319': 450, 'n008330': 451, 'n008353': 452, 'n008406': 453, 'n008418': 454, 'n008428': 455, 'n008460': 456, 'n008467': 457, 'n008490': 458, 'n008522': 459, 'n008531': 460, 'n008534': 461, 'n008623': 462, 'n008649': 463, 'n008667': 464, 'n008675': 465, 'n008693': 466, 'n008719': 467, 'n008750': 468, 'n008751': 469, 'n008786': 470, 'n008788': 471, 'n008795': 472, 'n008797': 473, 'n008842': 474, 'n008860': 475, 'n008881': 476, 'n008886': 477, 'n008906': 478, 'n008916': 479, 'n008950': 480, 'n008961': 481, 'n008975': 482, 'n008977': 483, 'n008982': 484, 'n008994': 485, 'n008996': 486, 'n009003': 487, 'n009049': 488, 'n009054': 489, 'n009061': 490, 'n009092': 491, 'n009095': 492, 'n009100': 493, 'n009142': 494, 'n009150': 495, 'n009179': 496, 'n009224': 497, 'n009230': 498, 'n009242': 499}\n",
      "训练数据集的长度为：10000\n",
      "测试数据集的长度为：500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): BasicBlock(\n",
      "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): BasicBlock(\n",
      "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
      ")\n",
      "--------第1轮训练开始---------\n",
      "使用GPU训练100次的时间为：5.5125508308410645\n",
      "训练次数：100,loss:0.0\n",
      "使用GPU训练100次的时间为：10.807164907455444\n",
      "训练次数：200,loss:0.0\n",
      "使用GPU训练100次的时间为：16.16376805305481\n",
      "训练次数：300,loss:0.0\n",
      "使用GPU训练100次的时间为：21.48072910308838\n",
      "训练次数：400,loss:0.0\n",
      "使用GPU训练100次的时间为：26.841686964035034\n",
      "训练次数：500,loss:0.0\n",
      "使用GPU训练100次的时间为：32.08529782295227\n",
      "训练次数：600,loss:0.0\n",
      "使用GPU训练100次的时间为：37.34609413146973\n",
      "训练次数：700,loss:0.0\n",
      "使用GPU训练100次的时间为：42.62132000923157\n",
      "训练次数：800,loss:0.0\n",
      "使用GPU训练100次的时间为：47.83115220069885\n",
      "训练次数：900,loss:0.0\n",
      "使用GPU训练100次的时间为：53.11292219161987\n",
      "训练次数：1000,loss:0.0\n",
      "使用GPU训练100次的时间为：58.36596202850342\n",
      "训练次数：1100,loss:0.0\n",
      "使用GPU训练100次的时间为：63.68899202346802\n",
      "训练次数：1200,loss:0.0\n",
      "使用GPU训练100次的时间为：69.07601308822632\n",
      "训练次数：1300,loss:0.0\n",
      "使用GPU训练100次的时间为：74.34032893180847\n",
      "训练次数：1400,loss:0.0\n",
      "使用GPU训练100次的时间为：79.67308497428894\n",
      "训练次数：1500,loss:0.0\n",
      "使用GPU训练100次的时间为：84.90556693077087\n",
      "训练次数：1600,loss:0.0\n",
      "使用GPU训练100次的时间为：90.12837600708008\n",
      "训练次数：1700,loss:0.0\n",
      "使用GPU训练100次的时间为：95.4009268283844\n",
      "训练次数：1800,loss:0.0\n",
      "使用GPU训练100次的时间为：100.76917600631714\n",
      "训练次数：1900,loss:0.0\n",
      "使用GPU训练100次的时间为：106.09289503097534\n",
      "训练次数：2000,loss:0.0\n",
      "使用GPU训练100次的时间为：111.31274604797363\n",
      "训练次数：2100,loss:0.0\n",
      "使用GPU训练100次的时间为：116.55828595161438\n",
      "训练次数：2200,loss:0.0\n",
      "使用GPU训练100次的时间为：121.83725595474243\n",
      "训练次数：2300,loss:0.0\n",
      "使用GPU训练100次的时间为：127.18637299537659\n",
      "训练次数：2400,loss:0.0\n",
      "使用GPU训练100次的时间为：132.63759207725525\n",
      "训练次数：2500,loss:0.0\n",
      "整体测试集上的loss：1.0679245553910732(越小越好,与上面的loss无关此为测试集的总loss)\n",
      "整体测试集上的正确率：0.0(越大越好)\n",
      "已修改模型\n",
      "--------第2轮训练开始---------\n",
      "使用GPU训练100次的时间为：5.261339902877808\n",
      "训练次数：2600,loss:0.1474381983280182\n",
      "使用GPU训练100次的时间为：10.518152952194214\n",
      "训练次数：2700,loss:0.0\n",
      "使用GPU训练100次的时间为：15.792593002319336\n",
      "训练次数：2800,loss:0.0\n",
      "使用GPU训练100次的时间为：21.103296756744385\n",
      "训练次数：2900,loss:0.0\n",
      "使用GPU训练100次的时间为：26.40718388557434\n",
      "训练次数：3000,loss:0.0\n",
      "使用GPU训练100次的时间为：31.659448862075806\n",
      "训练次数：3100,loss:0.0\n",
      "使用GPU训练100次的时间为：36.894272804260254\n",
      "训练次数：3200,loss:0.0\n",
      "使用GPU训练100次的时间为：42.17254590988159\n",
      "训练次数：3300,loss:0.0\n",
      "使用GPU训练100次的时间为：47.437958002090454\n",
      "训练次数：3400,loss:0.0\n",
      "使用GPU训练100次的时间为：52.75065588951111\n",
      "训练次数：3500,loss:0.0\n",
      "使用GPU训练100次的时间为：58.048911809921265\n",
      "训练次数：3600,loss:0.0\n",
      "使用GPU训练100次的时间为：63.40257906913757\n",
      "训练次数：3700,loss:0.0\n",
      "使用GPU训练100次的时间为：68.75054287910461\n",
      "训练次数：3800,loss:0.0\n",
      "使用GPU训练100次的时间为：74.09491991996765\n",
      "训练次数：3900,loss:0.0\n",
      "使用GPU训练100次的时间为：79.32434391975403\n",
      "训练次数：4000,loss:0.0\n",
      "使用GPU训练100次的时间为：84.5851559638977\n",
      "训练次数：4100,loss:0.03701819106936455\n",
      "使用GPU训练100次的时间为：89.85866498947144\n",
      "训练次数：4200,loss:0.0\n",
      "使用GPU训练100次的时间为：95.13888382911682\n",
      "训练次数：4300,loss:0.061037492007017136\n",
      "使用GPU训练100次的时间为：100.42013597488403\n",
      "训练次数：4400,loss:0.0\n",
      "使用GPU训练100次的时间为：105.6875228881836\n",
      "训练次数：4500,loss:0.0\n",
      "使用GPU训练100次的时间为：110.96839785575867\n",
      "训练次数：4600,loss:0.0\n",
      "使用GPU训练100次的时间为：116.32282090187073\n",
      "训练次数：4700,loss:0.0\n",
      "使用GPU训练100次的时间为：121.60052585601807\n",
      "训练次数：4800,loss:0.0\n",
      "使用GPU训练100次的时间为：126.86753487586975\n",
      "训练次数：4900,loss:0.2507082223892212\n",
      "使用GPU训练100次的时间为：132.13484597206116\n",
      "训练次数：5000,loss:0.0\n",
      "整体测试集上的loss：0.3518985934788361(越小越好,与上面的loss无关此为测试集的总loss)\n",
      "整体测试集上的正确率：0.0(越大越好)\n",
      "--------第3轮训练开始---------\n",
      "使用GPU训练100次的时间为：5.316644191741943\n",
      "训练次数：5100,loss:0.0\n",
      "使用GPU训练100次的时间为：10.652215242385864\n",
      "训练次数：5200,loss:0.0\n",
      "使用GPU训练100次的时间为：16.005924224853516\n",
      "训练次数：5300,loss:0.0\n",
      "使用GPU训练100次的时间为：21.30755615234375\n",
      "训练次数：5400,loss:0.0\n",
      "使用GPU训练100次的时间为：26.75537919998169\n",
      "训练次数：5500,loss:0.0\n",
      "使用GPU训练100次的时间为：32.60566020011902\n",
      "训练次数：5600,loss:0.0\n",
      "使用GPU训练100次的时间为：38.20730805397034\n",
      "训练次数：5700,loss:0.0\n",
      "使用GPU训练100次的时间为：43.69571304321289\n",
      "训练次数：5800,loss:0.0\n",
      "使用GPU训练100次的时间为：49.02875542640686\n",
      "训练次数：5900,loss:0.0\n",
      "使用GPU训练100次的时间为：54.35558342933655\n",
      "训练次数：6000,loss:0.0\n",
      "使用GPU训练100次的时间为：59.76848840713501\n",
      "训练次数：6100,loss:0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用GPU训练100次的时间为：65.24440431594849\n",
      "训练次数：6200,loss:0.0\n",
      "使用GPU训练100次的时间为：70.68306040763855\n",
      "训练次数：6300,loss:0.0\n",
      "使用GPU训练100次的时间为：76.24047017097473\n",
      "训练次数：6400,loss:0.0\n",
      "使用GPU训练100次的时间为：81.64021134376526\n",
      "训练次数：6500,loss:0.024114787578582764\n",
      "使用GPU训练100次的时间为：87.71177911758423\n",
      "训练次数：6600,loss:0.0\n",
      "使用GPU训练100次的时间为：94.04598021507263\n",
      "训练次数：6700,loss:0.0\n",
      "使用GPU训练100次的时间为：100.61912441253662\n",
      "训练次数：6800,loss:0.0\n",
      "使用GPU训练100次的时间为：107.16691517829895\n",
      "训练次数：6900,loss:0.0\n",
      "使用GPU训练100次的时间为：113.7373161315918\n",
      "训练次数：7000,loss:0.0\n",
      "使用GPU训练100次的时间为：120.24310827255249\n",
      "训练次数：7100,loss:0.0\n",
      "使用GPU训练100次的时间为：126.58569717407227\n",
      "训练次数：7200,loss:0.0\n",
      "使用GPU训练100次的时间为：132.8660752773285\n",
      "训练次数：7300,loss:0.0\n",
      "使用GPU训练100次的时间为：139.15867924690247\n",
      "训练次数：7400,loss:0.0\n",
      "使用GPU训练100次的时间为：145.46357226371765\n",
      "训练次数：7500,loss:0.0\n",
      "整体测试集上的loss：0.2624941846297588(越小越好,与上面的loss无关此为测试集的总loss)\n",
      "整体测试集上的正确率：0.0(越大越好)\n",
      "--------第4轮训练开始---------\n",
      "使用GPU训练100次的时间为：6.296970844268799\n",
      "训练次数：7600,loss:0.0\n",
      "使用GPU训练100次的时间为：12.591051816940308\n",
      "训练次数：7700,loss:0.0\n",
      "使用GPU训练100次的时间为：18.91730570793152\n",
      "训练次数：7800,loss:0.0\n",
      "使用GPU训练100次的时间为：25.244318962097168\n",
      "训练次数：7900,loss:0.002642592880874872\n",
      "使用GPU训练100次的时间为：31.552838802337646\n",
      "训练次数：8000,loss:0.0\n",
      "使用GPU训练100次的时间为：38.03350877761841\n",
      "训练次数：8100,loss:0.0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 189.92 MB, other allocations: 17.94 GB, max allowed: 18.13 GB). Tried to allocate 9.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 77>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m loss\u001b[38;5;241m=\u001b[39mloss_fn(outputs,targets)\n\u001b[1;32m     94\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()   \u001b[38;5;66;03m#梯度归零\u001b[39;00m\n\u001b[0;32m---> 95\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m         \u001b[38;5;66;03m#反向传播计算梯度\u001b[39;00m\n\u001b[1;32m     96\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()        \u001b[38;5;66;03m#梯度优化\u001b[39;00m\n\u001b[1;32m     98\u001b[0m total_train_step\u001b[38;5;241m=\u001b[39mtotal_train_step\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/torch-gpu/lib/python3.8/site-packages/torch/autograd/__init__.py:204\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    201\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    205\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: MPS backend out of memory (MPS allocated: 189.92 MB, other allocations: 17.94 GB, max allowed: 18.13 GB). Tried to allocate 9.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "print(\"是否使用GPU训练：{}\".format(torch.backends.mps.is_available()))    #打印是否采用gpu训练\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\");  #打印相应的gpu信息\n",
    "#数据增强太多也可能造成训练出不好的结果，而且耗时长，宜增强两三倍即可。\n",
    "normalize=transforms.Normalize(mean=[.5,.5,.5],std=[.5,.5,.5])  #规范化\n",
    "transform=transforms.Compose([                                  #数据处理\n",
    "    transforms.Resize((64,64)),\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "])\n",
    "dataset_train=ImageFolder('./data/train_sample',transform=transform)     #训练数据集\n",
    "# print(dataset_tran[0])\n",
    "dataset_valid=ImageFolder('./data/test_sample',transform=transform)     #验证或测试数据集\n",
    "# print(dataset_train.classer)#返回类别\n",
    "print(dataset_train.class_to_idx)                               #返回类别及其索引\n",
    "# print(dataset_train.imgs)#返回图片路径\n",
    "print(dataset_valid.class_to_idx)\n",
    "train_data_size=len(dataset_train)                              #放回数据集长度\n",
    "test_data_size=len(dataset_valid)\n",
    "print(\"训练数据集的长度为：{}\".format(train_data_size))\n",
    "print(\"测试数据集的长度为：{}\".format(test_data_size))\n",
    "#torch自带的标准数据集加载函数\n",
    "dataloader_train=DataLoader(dataset_train,batch_size=4,shuffle=True,num_workers=0,drop_last=True)\n",
    "dataloader_test=DataLoader(dataset_valid,batch_size=4,shuffle=True,num_workers=0,drop_last=True)\n",
    "\n",
    "#2.模型加载\n",
    "model_ft=models.resnet18(pretrained=True)#使用迁移学习，加载预训练权重\n",
    "print(model_ft)\n",
    "\n",
    "in_features=model_ft.fc.in_features\n",
    "model_ft.fc=nn.Sequential(nn.Linear(in_features,36),\n",
    "                          nn.Linear(36,6))#将最后的全连接改为（36，6），使输出为六个小数，对应六种植物的置信度\n",
    "#冻结卷积层函数\n",
    "# for i,para in enumerate(model_ft.parameters()):\n",
    "#     if i<18:\n",
    "#         para.requires_grad=False\n",
    "\n",
    "# print(model_ft)\n",
    "\n",
    "\n",
    "# model_ft.half()#可改为半精度，加快训练速度，在这里不适用\n",
    "\n",
    "\n",
    "\n",
    "# model_ft=model_ft.cuda()#将模型迁移到gpu\n",
    "\n",
    "model_ft = model_ft.to(device) #将模型迁移到gpu\n",
    "\n",
    "#3.优化器\n",
    "loss_fn=nn.CrossEntropyLoss()\n",
    "\n",
    "# loss_fn=loss_fn.cuda()  #将loss迁移到gpu\n",
    "\n",
    "loss_fn = loss_fn.to(device)  #将loss迁移到gpu\n",
    "\n",
    "learn_rate=0.01         #设置学习率\n",
    "optimizer=torch.optim.SGD(model_ft.parameters(),lr=learn_rate,momentum=0.01)#可调超参数\n",
    "\n",
    "total_train_step=0\n",
    "total_test_step=0\n",
    "epoch=50                #迭代次数\n",
    "writer=SummaryWriter(\"logs_train_yaopian\")\n",
    "best_acc=-1\n",
    "ss_time=time.time()\n",
    "\n",
    "for i in range(epoch):\n",
    "    start_time = time.time()\n",
    "    print(\"--------第{}轮训练开始---------\".format(i+1))\n",
    "    model_ft.train()\n",
    "    for data in dataloader_train:\n",
    "        imgs,targets=data\n",
    "        # if torch.cuda.is_available():\n",
    "        # imgs.float()\n",
    "        # imgs=imgs.float()#为上述改为半精度操作，在这里不适用\n",
    "#         imgs=imgs.cuda()\n",
    "        imgs = imgs.to(device)\n",
    "#         targets=targets.cuda()\n",
    "        targets = targets.to(device)\n",
    "        # imgs=imgs.half()\n",
    "        outputs=model_ft(imgs).to(device)\n",
    "        loss=loss_fn(outputs,targets)\n",
    "\n",
    "        optimizer.zero_grad()   #梯度归零\n",
    "        loss.backward()         #反向传播计算梯度\n",
    "        optimizer.step()        #梯度优化\n",
    "\n",
    "        total_train_step=total_train_step+1\n",
    "        if total_train_step%100==0:#一轮时间过长可以考虑加一个\n",
    "            end_time=time.time()\n",
    "            print(\"使用GPU训练100次的时间为：{}\".format(end_time-start_time))\n",
    "            print(\"训练次数：{},loss:{}\".format(total_train_step,loss.item()))\n",
    "            # writer.add_scalar(\"valid_loss\",loss.item(),total_train_step)\n",
    "    model_ft.eval()\n",
    "    total_test_loss=0\n",
    "    total_accuracy=0\n",
    "    with torch.no_grad():       #验证数据集时禁止反向传播优化权重\n",
    "        for data in dataloader_test:\n",
    "            imgs,targets=data\n",
    "            # if torch.cuda.is_available():\n",
    "            # imgs.float()\n",
    "            # imgs=imgs.float()\n",
    "            imgs = imgs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            # imgs=imgs.half()\n",
    "            outputs=model_ft(imgs)\n",
    "            loss=loss_fn(outputs,targets)\n",
    "            total_test_loss=total_test_loss+loss.item()\n",
    "            accuracy=(outputs.argmax(1)==targets).sum()\n",
    "            total_accuracy=total_accuracy+accuracy\n",
    "        print(\"整体测试集上的loss：{}(越小越好,与上面的loss无关此为测试集的总loss)\".format(total_test_loss))\n",
    "        print(\"整体测试集上的正确率：{}(越大越好)\".format(total_accuracy / len(dataset_valid)))\n",
    "\n",
    "        writer.add_scalar(\"valid_loss\",(total_accuracy/len(dataset_valid)),(i+1))#选择性使用哪一个\n",
    "        total_test_step = total_test_step + 1\n",
    "        if total_accuracy > best_acc:   #保存迭代次数中最好的模型\n",
    "            print(\"已修改模型\")\n",
    "            best_acc = total_accuracy\n",
    "            torch.save(model_ft, \"best_model_yaopian.pth\")\n",
    "ee_time=time.time()\n",
    "zong_time=ee_time-ss_time\n",
    "print(\"训练总共用时:{}h:{}m:{}s\".format(int(zong_time//3600),int((zong_time%3600)//60),int(zong_time%60))) #打印训练总耗时\n",
    "writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3ed5cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
